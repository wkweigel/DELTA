{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "### Imports ###\n",
    "###############\n",
    "\n",
    "from ugtm import eGTM, eGTC, eGTR, pcaPreprocess, runGTM, transform\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "from IPython.display import SVG\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from rdkit.Chem.PandasTools import ChangeMoleculeRendering\n",
    "\n",
    "from mhfp.encoder import MHFPEncoder\n",
    "\n",
    "\n",
    "##############\n",
    "### Inputs ###\n",
    "##############\n",
    "\n",
    "Training_Datafile='ChEMBL_10K.csv'\n",
    "\n",
    "Testing_Datafile='Notebook Outputs/Combined_DELs.csv' \n",
    "\n",
    "calc_fp=True\n",
    "\n",
    "#################\n",
    "### Functions ###\n",
    "#################\n",
    "\n",
    "def Generate_Fingerprinted_df(datafile, descriptor, nBits=512):\n",
    "     \n",
    "    \"\"\"Creates fingerprints of nBits for the smiles in the input datafile. Returns a df containing Smiles, IDs, DEL, Mols, and FPs.\"\"\"\n",
    "    \n",
    "\n",
    "    Pproc_df = pd.read_csv(datafile) #Create pre-processing df from datafile\n",
    "    Pproc_df['ROMol'] = Pproc_df.Smiles.apply(Chem.MolFromSmiles) #Create mols\n",
    "    Pproc_df['Smiles'] = Pproc_df.ROMol.apply(lambda x: Chem.MolToSmiles(x, kekuleSmiles=True, isomericSmiles=False)) #Cleanup the smiles\n",
    "    \n",
    "    if descriptor=='descriptors':\n",
    "        descr_list=[]\n",
    "        for smiles in Pproc_df['Smiles']:\n",
    "            #descriptor_group = from_smiles(smiles)\n",
    "            descr_list.append(list((from_smiles(smiles,descriptors=False, fingerprints=True)).values()))\n",
    "            if len(descr_list)>=20:\n",
    "                break\n",
    "        fp_array=np.array(descr_list)\n",
    "\n",
    "    if descriptor=='MXFP':\n",
    "        MXFP = mxfp.MXFPCalculator(dimensionality='2D')\n",
    "        fp = [MXFP.mxfp_from_mol(x) for x in Pproc_df['ROMol']]\n",
    "        fp_array=np.array(fp)\n",
    "        \n",
    "    if descriptor=='MHFP6':\n",
    "        MHFP6 = MHFPEncoder(n_permutations=nBits)\n",
    "        fp=[MHFP6.encode(x) for x in Pproc_df['Smiles']]\n",
    "        fp_array=np.array(fp)\n",
    "\n",
    "    if descriptor=='ECFP6':\n",
    "        ECFP6 = [AllChem.GetMorganFingerprintAsBitVect(x,radius=3, nBits=nBits) for x in Pproc_df['ROMol']]\n",
    "        fp_list=[list(l) for l in ECFP6]\n",
    "        fp_array = np.array(fp_list)\n",
    "    \n",
    "    if descriptor=='MACCS':\n",
    "        fp_list=[]\n",
    "        MACCS=[rdMolDescriptors.GetMACCSKeysFingerprint(x) for x in Pproc_df['ROMol']]\n",
    "        fp_str=[fp.ToBitString() for fp in MACCS]\n",
    "        for fp in fp_str:\n",
    "            temp_list=[int(x) for x in fp]\n",
    "            fp_list.append(temp_list)\n",
    "        fp_array = np.array(fp_list)\n",
    "\n",
    "    return(fp_array, Pproc_df)\n",
    "\n",
    "\n",
    "def _prepareMol(mol,kekulize):\n",
    "    \"\"\"Prepares mols for use in the moltosvg function\"\"\"\n",
    "    mc = Chem.Mol(mol.ToBinary())\n",
    "    if kekulize:\n",
    "        try:\n",
    "            Chem.Kekulize(mc)\n",
    "        except:\n",
    "            mc = Chem.Mol(mol.ToBinary())\n",
    "    if not mc.GetNumConformers():\n",
    "        rdDepictor.Compute2DCoords(mc)\n",
    "    return mc\n",
    " \n",
    "\n",
    "def moltosvg(mol,molSize=(225,100),kekulize=True,drawer=None,**kwargs):\n",
    "    \"\"\"Prepares svg imgs for use in inteactive plot visualizations.\"\"\"\n",
    "    mc = _prepareMol(mol,kekulize)\n",
    "    if drawer is None:\n",
    "        drawer = rdMolDraw2D.MolDraw2DSVG(molSize[0],molSize[1])\n",
    "    drawer.DrawMolecule(mc,**kwargs)\n",
    "    drawer.FinishDrawing()\n",
    "    svg = drawer.GetDrawingText()\n",
    "    return SVG(svg.replace('svg:',''))\n",
    "\n",
    "def pca_import(Dataset):\n",
    "    \"\"\"Import a csv containing PCA coords\"\"\"\n",
    "    pca_df=pd.read_csv(Dataset)\n",
    "    output_df=pca_df[['DEL', 'ID', 'Smiles']]\n",
    "    pca_array = np.array(pca_df[['pc1', 'pc2']].values.tolist())\n",
    "    #output_df['3Dpca'] = pca_df[['pc1', 'pc2', 'pc3']].values.tolist()\n",
    "    \n",
    "    return(pca_array, output_df)\n",
    "\n",
    "def pca_processing(Dataset):\n",
    "    \"\"\"Perform PCA using a txt file generated using the Fingerprinting_Utilities notebook. Needed only for large datasets.\"\"\"\n",
    "    raw_df=pd.read_csv(Dataset)\n",
    "    full_list=[]\n",
    "\n",
    "    #Process Fingerprints into ndarray\n",
    "    for idx, fp in enumerate(raw_df['FP']):\n",
    "        temp_list=fp.split(';')\n",
    "        temp_int_list= [eval(i) for i in temp_list]\n",
    "        full_list.append(temp_int_list)\n",
    "        \n",
    "    stack_arr= np.stack(full_list, axis=0)\n",
    "\n",
    "    #Perform PCA on Fingerprints\n",
    "    ###Use n_components=-1 for maximal variance. Note that PCA input ndarrays must all be the same size. \n",
    "    pca_coords=pcaPreprocess(stack_arr, doPCA=True,n_components=200)\n",
    "    return(pca_coords, raw_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess and compile the np arrays for the Training set and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "### Pre-Processing ###\n",
    "######################\n",
    "\n",
    "#Fingerprint all the molecules using ecfp6\n",
    "#Create the training and testing dataframes used as the input GTM datasets\n",
    "\n",
    "if calc_fp is True:\n",
    "    Training_fp, Training_df = Generate_Fingerprinted_df(Training_Datafile, descriptor='ECFP6',nBits=1024)\n",
    "    Testing_fp, Testing_df = Generate_Fingerprinted_df(Testing_Datafile, descriptor='ECFP6',nBits=1024)\n",
    "else:\n",
    "    Training_fp, Training_df = pca_import(Training_Datafile)\n",
    "    Testing_fp, Testing_df = pca_import(Testing_Datafile)\n",
    "\n",
    "#Create the training and testing arrays used for GTM\n",
    "X_train = np.array(Training_fp)\n",
    "X_test = np.array(Testing_fp)\n",
    "\n",
    "#Create label arrays for training and testing sets\n",
    "Train_labels = np.array(Training_df['DEL'])\n",
    "Test_labels = np.array(Testing_df['DEL'])\n",
    "\n",
    "#Create id arrays for training and testing sets\n",
    "Train_IDs = np.array(Training_df['ID'])\n",
    "Test_IDs = np.array(Testing_df['ID'])\n",
    "\n",
    "Train_smiles = np.array(Training_df['Smiles'])\n",
    "Test_smiles = np.array(Testing_df['Smiles'])\n",
    "\n",
    "# Additional Scaling (optional)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform GTM using Descrete Responsibility Patterns (RP)<br>\n",
    "NOTE: This calculation should take between 20-60 minutes depending on your machine specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "### G T M ###\n",
    "#############\n",
    "\n",
    "#Create a gtm training map \n",
    "trn_gtm = runGTM(X_train, k=50, m=22) \n",
    "\n",
    "#Project the test data onto the training map\n",
    "tst_gtm=transform(optimizedModel=trn_gtm, train=X_train, test=X_test,)\n",
    "\n",
    "#Create dataframe for the fit training array using column names \"x1\" and \"x2\"\n",
    "trn_chart_df=pd.DataFrame(trn_gtm.matMeans, columns=[\"x1\", \"x2\"])\n",
    "trn_chart_df['ID']=Train_IDs\n",
    "trn_chart_df['DEL']=Train_labels\n",
    "trn_chart_df['Smiles']=Train_smiles\n",
    "\n",
    "\n",
    "#Create dataframe for the projected array using column names \"x1\" and \"x2\"\n",
    "tst_chart_df=pd.DataFrame(tst_gtm.matMeans, columns=[\"x1\", \"x2\"])\n",
    "tst_chart_df['ID']=Test_IDs\n",
    "tst_chart_df['DEL']=Test_labels\n",
    "tst_chart_df['Smiles']=Test_smiles\n",
    "\n",
    "#Create dataframe combining the training and test arrays using the specified column names\n",
    "Overlay_df= pd.DataFrame(np.concatenate((tst_chart_df, trn_chart_df),axis=0), columns=['x1','x2','ID','DEL','Smiles'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================<br>\n",
    "Reference GTM completion times (Desktop PC, 12th Gen Intel CPU): <br>\n",
    "============================================<br>\n",
    "Training 50K ChEMBL  |  Testing 50K DEL2 50K DEL6  |   ECPF6 2048bits |  k=41 m=18  |  178 min<br>\n",
    "Training 50K ChEMBL  |  Testing 50K DEL2 50K DEL6  |   ECPF6 1024bits |  k=41 m=18  |  58 min <br>\n",
    "Training 50K ChEMBL  |  Testing 50K DEL2 50K DEL6  |   ECPF6 1024bits |  k=30 m=12  |  26 min<br>\n",
    "Training 50K ChEMBL  |  Testing 50K DEL2 50K DEL6  |   ECPF6 1024bits |  k=50 m=22  |  108 min<br>\n",
    "Training 10K ChEMBL  |  Testing 50K DEL2 50K DEL6  |   ECPF6 1024bits |  k=50 m=22  |  18 min<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output training and test data to csv (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Overlay_df.to_csv('Notebook Outputs/gtm_output.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "### Plotting ###\n",
    "################\n",
    "\n",
    "# Plot of training set colored by labels\n",
    "trn_chart = alt.Chart(trn_chart_df).mark_circle().encode(\n",
    "    x='x1',y='x2',\n",
    "    size=alt.value(10),\n",
    "    color=alt.Color(\"DEL:N\",\n",
    "           legend=alt.Legend(title=\"Type\")),\n",
    "    #opacity=\"probability_of_predominant_class:Q\",\n",
    "    tooltip=[\"x1\", \"x2\", \"DEL:N\", 'ID:N']\n",
    ").properties(title=\"Training Set\", width=300, height=300).interactive()\n",
    "\n",
    "# Projection of test set colored by label\n",
    "tst_chart = alt.Chart(tst_chart_df).mark_circle(size=5).encode(\n",
    "    x='x1', y='x2',\n",
    "    color=alt.Color(\"DEL:N\",\n",
    "                    legend=alt.Legend(title=\"Type\")),\n",
    "    size=alt.value(10),\n",
    "    tooltip=[\"x1\", \"x2\", \"DEL:N\", 'ID:N']\n",
    ").properties(title=\"Projected Sets\", width=300, height=300).interactive()\n",
    "\n",
    "# Seperate the projected points into diffrent plots based on \"DEL\" classification\n",
    "Individual_charts = alt.Chart(tst_chart_df).mark_circle().encode(\n",
    "    x='x1', y='x2',\n",
    "    color=alt.Color(\"DEL:N\",\n",
    "                    legend=alt.Legend(title=\"Type\")),\n",
    "    size=alt.value(10),\n",
    "    tooltip=[\"x1\", \"x2\", \"DEL:N\", 'ID:N']\n",
    ").properties(title=\"Projected Set\", width=300, height=300).facet(\n",
    "    column='DEL:N'\n",
    ").interactive()\n",
    "\n",
    "# Create an overlay of the training and test charts\n",
    "overlay_chart = trn_chart + tst_chart\n",
    "\n",
    "#Plot the training set heatmap\n",
    "Trained_heatmap= alt.Chart(trn_chart_df).mark_rect().encode(\n",
    "    alt.X('x1:Q', bin=alt.Bin(maxbins=200)),\n",
    "    alt.Y('x2:Q', bin=alt.Bin(maxbins=200)),\n",
    "    alt.Color('count():Q', scale=alt.Scale(scheme='turbo', domain=[0, 10])),\n",
    ").properties(title=\"Training Set\", width=300, height=300).interactive()\n",
    "\n",
    "\n",
    "#Plot the projected heatmaps seperatley (faceted)\n",
    "Projected_heatmaps= alt.Chart(tst_chart_df).mark_rect().encode(\n",
    "    alt.X('x1:Q', bin=alt.Bin(maxbins=200)),\n",
    "    alt.Y('x2:Q', bin=alt.Bin(maxbins=200)),\n",
    "    alt.Color('count():Q', scale=alt.Scale(scheme='turbo', domain=[0, 10]))\n",
    ").properties(title=\"Projected Set\", width=300, height=300).facet(\n",
    "    column='DEL:N'\n",
    ").interactive()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the GTM Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.data_transformers.disable_max_rows()\n",
    "plots = trn_chart | Individual_charts\n",
    "heatmaps = Trained_heatmap | Projected_heatmaps\n",
    "\n",
    "alt.vconcat(plots, heatmaps)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rdkit-39env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e366512f30e682182766d08f53ea8d4cdd294e5567b4ceb9191014b5bc5ef3fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
